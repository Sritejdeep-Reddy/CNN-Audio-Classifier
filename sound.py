# -*- coding: utf-8 -*-
"""sound.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1IiqWus_xnIPxK8S23x42o8rCK4qVehhL
"""

from google.colab import drive
drive.mount('/content/drive')

import pandas as pd
import numpy as np
import matplotlib.pylab as plt
import seaborn as sns
import glob #for reading names of all files in a directory
import os

import librosa
import librosa.display
import IPython.display as ipd

from itertools import cycle
# from nnAudio import Spectrogram

import torch
import torchaudio
import torch.nn.functional as F
import torchaudio.transforms as T

def plot_spectrogram(spec, title=None, ylabel='freq_bin', aspect='auto', xmax=None, save_path=None):
    fig, axs = plt.subplots(1, 1)
    im = axs.imshow(librosa.power_to_db(spec), origin='lower', aspect=aspect)
    axs.axis('off')

    fig.canvas.draw()
    image_array = np.array(fig.canvas.renderer.buffer_rgba())
    plt.close(fig)
    # image_array= image_array[60:-55 ,80:-65,0:3]

    return image_array

path_train = "/content/drive/MyDrive/DL assignment/audio_dataset/train/*" #Change path acc to system
categories_train = glob.glob(path_train)
categories_train.sort()
print(categories_train)

path_val = "/content/drive/MyDrive/DL assignment/audio_dataset/val/*" #Change path acc to system
categories_val = glob.glob(path_val)
categories_val.sort()
print(categories_val)

# %%capture
label=-1
target_len = 176400
train_x = []
train_y = []
#Each class - loop
for _ in range(7):

    #One class
    for i in categories_train[_:_+1]:
        files = glob.glob(i+ "/*")
        label += 1

        for j in files:
            waveform, sample_rate = torchaudio.load(j)

            #combining all channels into one
            channels = []
            for k in range(waveform.size(0)):
                channels.append(waveform[k])

            final_waveform = torch.stack(channels).mean(dim=0)

            #Time-Stretching
            stretched = librosa.effects.time_stretch(final_waveform.numpy(), rate=final_waveform.shape[0]/target_len)
            stretched = stretched[:target_len:]
            stretched = torch.tensor(stretched)

            #Gammatone
            n_fft = 2048
            win_length = None
            hop_length = 1024
            n_mels = 128

            mel_spectrogram = T.MelSpectrogram(
            sample_rate=sample_rate,
                n_fft=n_fft,
                win_length=win_length,
                hop_length=hop_length,
                center=True,
                pad_mode="reflect",
                power=2.0,
                norm='slaney',
                onesided=True,
                n_mels=n_mels,
                mel_scale="htk"
            );

            train_x.append(plot_spectrogram(mel_spectrogram(stretched)))
            train_y.append(label)

np.save("/content/drive/MyDrive/DL assignment/audio_dataset/train_Mel_x_1",train_x)
np.save("/content/drive/MyDrive/DL assignment/audio_dataset/train_Mel_y_1",train_y)

# %%capture
label=6
target_len = 176400
train_x = []
train_y = []
#Each class - loop
for _ in range(7,10):

    #One class
    for i in categories_train[_:_+1]:
        files = glob.glob(i+ "/*")
        label += 1

        for j in files:
            waveform, sample_rate = torchaudio.load(j)
            # waveform = waveform.to(device)

            #combining all channels into one
            channels = []
            for k in range(waveform.size(0)):
                channels.append(waveform[k])

            final_waveform = torch.stack(channels).mean(dim=0)

            #Time-Stretching
            stretched = librosa.effects.time_stretch(final_waveform.numpy(), rate=final_waveform.shape[0]/target_len)
            stretched = stretched[:target_len:]
            stretched = torch.tensor(stretched)

            #Gammatone
            n_fft = 2048
            win_length = None
            hop_length = 1024
            n_mels = 128

            mel_spectrogram = T.MelSpectrogram(
            sample_rate=sample_rate,
                n_fft=n_fft,
                win_length=win_length,
                hop_length=hop_length,
                center=True,
                pad_mode="reflect",
                power=2.0,
                norm='slaney',
                onesided=True,
                n_mels=n_mels,
                mel_scale="htk"
            );

            train_x.append(plot_spectrogram(mel_spectrogram(stretched)))
            train_y.append(label)

np.save("/content/drive/MyDrive/DL assignment/audio_dataset/train_Mel_x_2",train_x)
np.save("/content/drive/MyDrive/DL assignment/audio_dataset/train_Mel_y_2",train_y)

# %%capture
label=9
target_len = 176400
train_x = []
train_y = []
#Each class - loop
for _ in range(10,13):

    #One class
    for i in categories_train[_:_+1]:
        files = glob.glob(i+ "/*")
        label += 1

        for j in files:
            waveform, sample_rate = torchaudio.load(j)
            # waveform = waveform.to(device)

            #combining all channels into one
            channels = []
            for k in range(waveform.size(0)):
                channels.append(waveform[k])

            final_waveform = torch.stack(channels).mean(dim=0)

            #Time-Stretching
            stretched = librosa.effects.time_stretch(final_waveform.numpy(), rate=final_waveform.shape[0]/target_len)
            stretched = stretched[:target_len:]
            stretched = torch.tensor(stretched)

            #Gammatone
            n_fft = 2048
            win_length = None
            hop_length = 1024
            n_mels = 128

            mel_spectrogram = T.MelSpectrogram(
            sample_rate=sample_rate,
                n_fft=n_fft,
                win_length=win_length,
                hop_length=hop_length,
                center=True,
                pad_mode="reflect",
                power=2.0,
                norm='slaney',
                onesided=True,
                n_mels=n_mels,
                mel_scale="htk"
            );

            train_x.append(plot_spectrogram(mel_spectrogram(stretched)))
            train_y.append(label)

np.save("/content/drive/MyDrive/DL assignment/audio_dataset/train_Mel_x_3",train_x)
np.save("/content/drive/MyDrive/DL assignment/audio_dataset/train_Mel_y_3",train_y)

# %%capture
label=-1
target_len = 176400
train_x = []
train_y = []
#Each class - loop
for _ in range(13):

    #One class
    for i in categories_val[_:_+1]:
        files = glob.glob(i+ "/*")
        label += 1

        for j in files:
            waveform, sample_rate = torchaudio.load(j)
            # waveform = waveform.to(device)

            #combining all channels into one
            channels = []
            for k in range(waveform.size(0)):
                channels.append(waveform[k])

            final_waveform = torch.stack(channels).mean(dim=0)

            #Time-Stretching
            stretched = librosa.effects.time_stretch(final_waveform.numpy(), rate=final_waveform.shape[0]/target_len)
            stretched = stretched[:target_len:]
            stretched = torch.tensor(stretched)

            #Gammatone
            n_fft = 2048
            win_length = None
            hop_length = 1024
            n_mels = 128

            mel_spectrogram = T.MelSpectrogram(
            sample_rate=sample_rate,
                n_fft=n_fft,
                win_length=win_length,
                hop_length=hop_length,
                center=True,
                pad_mode="reflect",
                power=2.0,
                norm='slaney',
                onesided=True,
                n_mels=n_mels,
                mel_scale="htk"
            );

            train_x.append(plot_spectrogram(mel_spectrogram(stretched)))
            train_y.append(label)

np.save("/content/drive/MyDrive/DL assignment/audio_dataset/val_Mel_x_1",train_x)
np.save("/content/drive/MyDrive/DL assignment/audio_dataset/val_Mel_y_1",train_y)



!pip install wandb onnx -Uq
!pip install wandb --upgrade

import wandb

wandb.login()





from google.colab import drive
drive.mount('/content/drive')

import glob
import numpy as np
import matplotlib.pyplot as plt
from PIL import Image
import os

import torch
import torchaudio
import torch.nn as nn
import torch.nn.functional as F
import torchaudio.transforms as T
import torch.optim as optim
from torch.utils.data import DataLoader, Dataset
from tqdm.notebook import tqdm

path = "/content/drive/MyDrive/DL assignment/audio_dataset/train_mel/x/*" #Change path acc to system
categories_x = glob.glob(path)
categories_x.sort()
categories_x

path = "/content/drive/MyDrive/DL assignment/audio_dataset/train_mel/y/*" #Change path acc to system
categories_y = glob.glob(path)
categories_y.sort()
categories_y

train_x = []
train_y = []
for i in categories_x:
    temp = np.load(i);
    for k in range(len(temp)):
        # print(temp[k])
        a = torch.tensor(temp[k])
        train_x.append(a)

for i in categories_y:
    temp = np.load(i);
    for k in range(len(temp)):
        a = torch.tensor(temp[k])
        train_y.append(a)

path = "/content/drive/MyDrive/DL assignment/audio_dataset/val_mel/x/*" #Change path acc to system
categories_x = glob.glob(path)
categories_x.sort()
categories_x


path = "/content/drive/MyDrive/DL assignment/audio_dataset/val_mel/y/*" #Change path acc to system
categories_y = glob.glob(path)
categories_y.sort()
categories_y

val_x = []
val_y = []
for i in categories_x:
    temp = np.load(i);
    for k in range(len(temp)):
        a = torch.tensor(temp[k])
        val_x.append(a)

for i in categories_y:
    temp = np.load(i);
    for k in range(len(temp)):
        a = torch.tensor(temp[k])
        val_y.append(a)

categories_x

label = torch.as_tensor(train_y)
labels,weights = torch.unique(label,return_counts=True)
weights = weights.sum()/weights
weights

class dataset(Dataset):
    def __init__(self, x, y):
        self.x = x
        self.y = y

    def __len__(self):
        return len(self.x)

    def __getitem__(self, idx):
        return self.x[idx], self.y[idx]

train_data = dataset(train_x, train_y)
val_data = dataset(val_x, val_y)
train_loader = DataLoader(train_data, batch_size=64, shuffle=True)
test_loader = DataLoader(val_data, batch_size=1, shuffle=True)

class CNN(nn.Module):

    def __init__(self):
        super(CNN, self).__init__()

        self.relu = nn.ReLU()
        self.layer1 = nn.Sequential(
            nn.Conv2d(in_channels=4, out_channels=32, kernel_size=5, padding=0, stride = 1),
            nn.BatchNorm2d(32),
            nn.ReLU(),
            nn.MaxPool2d(kernel_size=2, stride=2),
            nn.Dropout(0.25)
        )
        self.layer2 = nn.Sequential(
            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=5, padding =0, stride=1),
            nn.BatchNorm2d(64),
            nn.ReLU(),
            nn.MaxPool2d(2),
            nn.Dropout(0.3)
        )
        self.layer3 = nn.Sequential(
            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=5, padding =1, stride=1),
            nn.BatchNorm2d(128),
            nn.ReLU(),
            nn.MaxPool2d(kernel_size=2, stride=2),
            nn.Dropout(0.3)
        )
        self.layer4 = nn.Sequential(
            nn.Conv2d(in_channels=128, out_channels=64, kernel_size=5, padding =0, stride=1),
            nn.BatchNorm2d(64),
            nn.ReLU(),
            nn.MaxPool2d(kernel_size=2, stride=2),
            nn.Dropout(0.3)
        )
        self.layer5 = nn.Sequential(
            nn.Conv2d(in_channels=64, out_channels=32, kernel_size=3, padding =0, stride=1),
            nn.BatchNorm2d(32),
            nn.ReLU(),
            nn.MaxPool2d(kernel_size=2, stride=2),
            nn.Dropout(0.2)
        )
        self.layer6 = nn.Sequential(
            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding =0, stride=1),
            nn.BatchNorm2d(64),
            nn.ReLU(),
            nn.MaxPool2d(kernel_size=2, stride=2),
            nn.Dropout(0.25)
        )
        self.res1 = nn.Conv2d(in_channels=4, out_channels=64, kernel_size=1, bias = False, stride=1)
        self.fc1 = nn.Linear(in_features=2240, out_features=640)
        self.drop = nn.Dropout(0.2)
        self.fc2 = nn.Linear(in_features=640, out_features=320)
        self.fc3 = nn.Linear(in_features=320, out_features=160)
        self.fc4 = nn.Linear(in_features=160, out_features=13)
    def forward(self, x):
        y = x
        x = self.layer1(x)
        x = self.layer2(x)
        x = self.layer3(x)
        x = self.layer4(x)
        x = self.layer5(x)
        x = self.layer6(x)
        x = x.view(x.size(0), -1)
        x = self.fc1(x)
        x = self.drop(x)
        x = self.relu(x)
        x = self.fc2(x)
        x = self.drop(x)
        x = self.relu(x)
        x = self.fc3(x)
        x = self.drop(x)
        x = self.relu(x)
        x = self.fc4(x)
        return x

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

run = wandb.init(
    # Set the project where this run will be logged
    project="Sound CNN",
    # Track hyperparameters and run metadata
    config={
        "learning_rate": 0.001,
        "epochs": 10,
    },
)

# del model
# torch.cuda.empty_cache()

model = CNN().to(device)
optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-6)
criterion = nn.CrossEntropyLoss(weight = weights).to(device)
epochs = 30
model.train()


for epoch in tqdm(range(epochs)):
    print(f"Epoch:{epoch+1}")
    total_loss = 0
    for batch_idx, (data, target) in tqdm(enumerate(train_loader),desc=f'Epoch {epoch + 1}/{epochs}', unit='batch'):
        # print(data.shape)
        data = data.permute(0,3,2,1)
        data = data.float().to(device)
        target = target.to(device)
        optimizer.zero_grad()
        output = model(data)
        loss = criterion(output, target)
        loss.backward()
        total_loss += loss.item()
        if(batch_idx)%30==0:
            print(f"Batch_idx = {batch_idx},Loss = {total_loss}")
        optimizer.step()
    print(f"Epoch = {epoch+1},Loss = {total_loss}\n\n")
    wandb.log({'Epoch':epoch,
        'trainloss':total_loss
        })
print('Finished Training')

model.eval()
# Run the model on some test examples
with torch.no_grad():
    correct, total = 0, 0
    for images, labels in test_loader:
        images = images.float()
        images = images.permute(0,3,2,1)
        # labels = labels.float()

        images, labels = images.to(device), labels.to(device)
        outputs = model(images)
        _, predicted = torch.max(outputs.data, 1)
        # print(outputs.data)
        total += labels.size(0)
        # print(predicted)
        # print(labels)
        # labels = torch.argmax(labels, dim=1)
        # labels = labels.view(1,-1)
        correct += (predicted == labels).sum().item()

    print(f"Accuracy of the model on the {total} " +
        f"test images: {correct / total:%}")

torch.save(model, "/content/drive/MyDrive/DL assignment/audio_dataset/model_CNN3.pt")

total_params_model_1 = sum(p.numel() for p in model.parameters())
total_params_model_1







class CNN(nn.Module):

    def __init__(self):
        super(CNN, self).__init__()

        self.relu = nn.ReLU()
        self.layer1 = nn.Sequential(
            nn.Conv2d(in_channels=4, out_channels=16, kernel_size=3, padding=1, stride = 1),
            nn.BatchNorm2d(16),
            nn.ReLU(),
            # nn.MaxPool2d(kernel_size=2, stride=2),
            nn.Dropout(0.25)
        )
        self.layer2 = nn.Sequential(
            nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, padding =1, stride=1),
            nn.BatchNorm2d(32),
            nn.ReLU(),
            # nn.MaxPool2d(2),
            nn.Dropout(0.3)
        )
        self.layer3 = nn.Sequential(
            nn.Conv2d(in_channels=32, out_channels=32, kernel_size=5, padding =0, stride=1),
            nn.BatchNorm2d(32),
            nn.ReLU(),
            nn.MaxPool2d(kernel_size=2, stride=2),
            nn.MaxPool2d(kernel_size=2, stride=2),
            nn.MaxPool2d(kernel_size=2, stride=2),
            nn.Dropout(0.3)
        )
        self.layer4 = nn.Sequential(
            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=5, padding =0, stride=1),
            nn.BatchNorm2d(64),
            nn.ReLU(),
            nn.MaxPool2d(kernel_size=2, stride=2),
            nn.MaxPool2d(kernel_size=2, stride=2),
            nn.MaxPool2d(kernel_size=2, stride=2),
            nn.Dropout(0.3)
        )
        self.res1 = nn.Conv2d(in_channels=4, out_channels=32, kernel_size=1, bias = False, stride=1)
        self.fc1 = nn.Linear(in_features=3456, out_features=640)
        self.drop = nn.Dropout(0.2)
        self.fc2 = nn.Linear(in_features=640, out_features=320)
        self.fc3 = nn.Linear(in_features=320, out_features=160)
        self.fc4 = nn.Linear(in_features=160, out_features=13)
    def forward(self, x):
        y = x
        x = self.layer1(x)
        x = self.layer2(x)
        x = x + self.res1(y)
        x = self.layer3(x)
        x = self.layer4(x)
        x = x.view(x.size(0), -1)
        x = self.fc1(x)
        x = self.drop(x)
        x = self.relu(x)
        x = self.fc2(x)
        x = self.drop(x)
        x = self.relu(x)
        x = self.fc3(x)
        x = self.drop(x)
        x = self.relu(x)
        x = self.fc4(x)
        return x

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

train_loader = DataLoader(train_data, batch_size=16, shuffle=True)
test_loader = DataLoader(val_data, batch_size=1, shuffle=True)

run = wandb.init(
    # Set the project where this run will be logged
    project="Sound CNN",
    # Track hyperparameters and run metadata
    config={
        "learning_rate": 0.001,
        "epochs": 10,
    },
)

model = CNN().to(device)
optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-6)
criterion = nn.CrossEntropyLoss(weight = weights).to(device)
epochs = 10
model.train()


for epoch in tqdm(range(epochs)):
    print(f"Epoch:{epoch+1}")
    total_loss = 0
    for batch_idx, (data, target) in tqdm(enumerate(train_loader),desc=f'Epoch {epoch + 1}/{epochs}', unit='batch'):
        # print(data.shape)
        data = data.permute(0,3,2,1)
        data = data.float().to(device)
        target = target.to(device)
        optimizer.zero_grad()
        output = model(data)
        loss = criterion(output, target)
        loss.backward()
        total_loss += loss.item()
        if(batch_idx)%30==0:
            print(f"Batch_idx = {batch_idx},Loss = {total_loss}")
        optimizer.step()
    print(f"Epoch = {epoch+1},Loss = {total_loss}\n\n")
    wandb.log({'Epoch':epoch,
        'trainloss':total_loss
        })
print('Finished Training')

model.eval()
# Run the model on some test examples
with torch.no_grad():
    correct, total = 0, 0
    for images, labels in test_loader:
        images = images.float()
        images = images.permute(0,3,2,1)

        images, labels = images.to(device), labels.to(device)
        outputs = model(images)
        _, predicted = torch.max(outputs.data, 1)
        # print(outputs.data)
        total += labels.size(0)
        correct += (predicted == labels).sum().item()

    print(f"Accuracy of the model on the {total} " +
        f"test images: {correct / total:%}")

torch.save(model, "/content/drive/MyDrive/DL assignment/audio_dataset/model_CNN2.pt")

total_params_model_2 = sum(p.numel() for p in model.parameters())
total_params_model_2

